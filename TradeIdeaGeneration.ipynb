{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "The dataset represents the stocks with market capitalization higher than 3000 mil USD. The dataset was retrieved using the stock screener of ZACKS.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./zacks_stocks_custom_screen_2024-08-21.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s just rename those columns so we can more easily refer to them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPS0 = Earning Per Share in year 0 (last year)\n",
    "# EPS1 = Earning Per Share in year 1 (current year)\n",
    "# EPS2 = Earning Per Share in year 2 (next year)\n",
    "df = df.rename(columns={'Last Close':'Price', 'Last Yr`s EPS (F0) Before NRI':'EPS0', 'F1 Consensus Est.':'EPS1', 'F2 Consensus Est.':'EPS2'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange the columns in way that on we can interpret them better. We move columns 'Market Cap (mil)', 'Price', 'EPS0', 'EPS1' and 'EPS2' are shown next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Company Name', 'Ticker', 'Month of Fiscal Yr End', 'Last Reported Fiscal Yr  (yyyymm)', 'Sector', 'Industry', 'Exchange', 'Market Cap (mil)', 'Price', 'EPS0', 'EPS1', 'EPS2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the dataset and undestand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the length of the dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output statistical description of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the proportion of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 66 company names are missing but since no ticker is missing this doesn't bother us as the ticker is a unique identifier for each stock. \n",
    "\n",
    "We also see that 97 entries for EPS0 (Earning per Share in the year 0) are missing this is roughly 5% of our data set. We also see that rougly 200 entries for both EPS1 and EPS2 are missing. These respresent each roughly 10% of our dataset. If we assume that there is no intersection of the missing values for EPS0, EPS1 and EPS2 the missing values would accout tofether for roughly 25% of our data set.\n",
    "\n",
    "Let's clean the dataset from these missing values and check the real effect on the dataframe and decide if we can live with that or we will need to find more sophisticated ways to handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['EPS0'].notna() & df['EPS1'].notna() & df['EPS2'].notna()]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the cleaned dataset contains 1802 stocks which is 10% less than the original dataset with 2028 stocks. For the purpose of our data analysis this subset carries more than enough evidence to identify which industries are growing/contracting most and to identify potential stocks outliers which are especially positive/negative. Therefore, we will continue with the obtained dataset without further investigation of the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final check shows us that our dataset is clean of missing values exept for one company name value but we don't care about that since we have the Ticker instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have finished our data set setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Goal 1\n",
    "# Identify Sectors & Industries with highest Earning Growth/Contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rank Sectors according to Earning Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first busieness goal in this section is to understand which Sectors are expected to have the strongest Grwoth/Contraction in current and next year according to the Forward Looking Earning Growth Expectation for year 1 and year 2 (EG1 and EG2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will add some columns to our dataframe which represents useful financial metrics and prepare the dataset for ranking according to earning growth.\n",
    "\n",
    "The colums which we will all are derived by simple addition/division of the stock price, EPS0, EPS1 and EPS2 values we already have in our dataframe. First, We add two columns \"EG1\" i.e. earning grwoth in year 1 and \"EG2\" this is earning growth in year 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earning_growth(eps0, eps1):\n",
    "    '''\n",
    "    INPUT - eps0 - flaot - with the earning per share for year 0\n",
    "            eps1 - flaot - with the earning per share for year 1\n",
    "    OUTPUT - \n",
    "            eg - float - earning per share growth from year 0 to year 1\n",
    "    '''\n",
    "    if eps0<0:\n",
    "        if eps1 >= 0:\n",
    "            return 1.0\n",
    "    if eps0==0:\n",
    "        if eps1>0:\n",
    "            return 1.0\n",
    "        if eps1<0:\n",
    "            return -1.0\n",
    "        if eps==0:\n",
    "            return 0\n",
    "        \n",
    "    eg=(eps1-eps0)/abs(eps0)\n",
    "    \n",
    "    return eg\n",
    "\n",
    "df['EG1']= df.apply(lambda row: earning_growth(row['EPS0'], row['EPS1']), axis=1)\n",
    "\n",
    "df['EG2']= df.apply(lambda row: earning_growth(row['EPS1'], row['EPS2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore the struture of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns have been successfully added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the columns below which we will need later in our analysis:\n",
    "    - \"PE1\" and \"PE2\" i.e. price to earning ratios for current and next year respectively. \n",
    "    - \"PEG1\" and \"PEG2\" i.e. price to earning growth ratios for current and next year respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE1 = (df['Price']/df['EPS1'])\n",
    "df['PE1']=PE1\n",
    "\n",
    "PE2 = (df['Price']/df['EPS2'])\n",
    "df['PE2']=PE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEG1 = (df['PE1']/(df['EG1'])*100)\n",
    "df['PEG1']=PEG1\n",
    "\n",
    "PEG2 = (df['PE2']/(df['EG2'])*100)\n",
    "df['PEG2']=PEG2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the newly added columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the description table above we can see that the EG1 and EG2 columns have no division by zero or any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see also see that we have EG1 and EG2 max values of respectively 16.2 and 287.0. Since these values are to be interpreted as percentage values they mean a maximum earning growth of 1620% in year 1 and of 28700% in year 2. These values are extremly high and unrealistic. Probably, they result from a division by a very small number (Law of Small Numbers) or from a wrong entry.\n",
    "\n",
    "Similarly, we see that we have EG1 and EG2 min values of -23.0 and -4.72 or -2300% and -472%\n",
    "\n",
    "For the purpose of our analysis, we will consider only growth rate between -400% and +400% or between -4.0 and +4.0 to be realistic. All other values will be considered as outliers to be deleted from the main dataset \"df\" to be added to the dataframe \"outliers\" for further investigation in later analysis stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the stocks with EG1>threshold or EG1<-threshold for later analysis\n",
    "threshold = 4\n",
    "outliers = df[df['EG1']>threshold]\n",
    "outliers = outliers.append(df[df['EG1']<-threshold])\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the stockswith EG1>threshold or EG1<-threshold from the dataset\n",
    "df = df[~(df['EG1']>threshold)]\n",
    "df = df[~(df['EG1']<-threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the stocks with EG2>threshold or EG2<-threshold for later analysis\n",
    "outliers = outliers.append(df[df['EG2']>threshold])\n",
    "outliers = outliers.append(df[df['EG2']<-threshold])\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Rank Sectors according to Forward Looking Earning Growth Expectation for current year (EG1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are done witht the preparation of the dataset and we will start the ranking according to earning growth 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the stocks with EG2>threshold or EG2<-threshold from the dataset\n",
    "df = df[~(df['EG2']>threshold)]\n",
    "df = df[~(df['EG2']<-threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will group the data set by Sector and calculate the average EG1 for each sector as well as calculate the 50% and 75% quantiles for each sector to undestand the sensitivity of the calculared averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_sector = df.loc[:, ['Sector', 'EG1', 'EG2']].groupby('Sector')\n",
    "\n",
    "avgs = group_by_sector.EG1.mean()\n",
    "avgs = avgs.rename('mean EG1')\n",
    "qtls50 = group_by_sector.EG1.quantile(0.5)\n",
    "qtls50 = qtls50.rename('50% EG1')\n",
    "qtls75 = group_by_sector.EG1.quantile(0.75)\n",
    "qtls75 = qtls75.rename('75% EG1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all the calculation of each sector let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean earning growth mean for year 1 across sectors\n",
    "avgs.plot(kind='barh', legend=\"Mean\")\n",
    "plt.title(\"Mean EG1 across Sectors\")\n",
    "plt.show()\n",
    "\n",
    "# plot the 50% earning growth for year 1 across sectors \n",
    "\n",
    "qtls50.plot(kind='barh', legend=\"75Q\")\n",
    "plt.title(\"50%P EG1 across Sectors\")\n",
    "plt.show()\n",
    "\n",
    "# plot the 75% earning growth for year 1 across sectors \n",
    "\n",
    "qtls75.plot(kind='barh', legend=\"75Q\")\n",
    "plt.title(\"75%P EG1 across Sectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above for Sector Rankings accroding to EG1 allows us to get a first glimpse of which industries are expecting strongest Growth/Contraction in current year.\n",
    "\n",
    "Though the plot gives us a first impression of where strongest Grwoth/Contraction is to be expected in the current year, we aknowledge that there are big differences between the mean, 50% quantile and 75% quantile plots. This indicates that the EG1 distribution within most sectors is realtively unhomogeneous.\n",
    "\n",
    "So, let's see if a ranking according to according to Forward Looking Earning Growth Expectation for next year (EG2) will give a different picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Rank Sectors according to Forward Looking Earning Growth Expectation for next year (EG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = group_by_sector.EG2.mean()\n",
    "avgs = avgs.rename('mean EG2')\n",
    "qtls50 = group_by_sector.EG2.quantile(0.5)\n",
    "qtls50 = qtls50.rename('50% EG2')\n",
    "qtls75 = group_by_sector.EG2.quantile(0.75)\n",
    "qtls75 = qtls75.rename('75% EG2')\n",
    "\n",
    "# plot the mean earning growth mean for year 2 across sectors\n",
    "avgs.plot(kind='barh', legend=\"Mean\")\n",
    "plt.title(\"Mean EG2 across Sectors\")\n",
    "plt.show()\n",
    "\n",
    "# plot the 50% earning growth for year 2 across sectors\n",
    "\n",
    "qtls50.plot(kind='barh', legend=\"75Q\")\n",
    "plt.title(\"50%P EG2 across Sectors\")\n",
    "plt.show()\n",
    "\n",
    "# plot the 75% earning growth for year 2 across sectors\n",
    "\n",
    "qtls75.plot(kind='barh', legend=\"75Q\")\n",
    "plt.title(\"75%P EG2 across Sectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above for Sector Rankings accroding to EG2 gives us an second impression about which sectors are expected to grow/contract most in year 2 (next year).\n",
    "\n",
    "The differences between the mean, 50% quantile and 75% quantile plots for EG2 are smaller than those for EG1. This indicates that the EG2 distribution within most sectors is more homogeneously distributed than the EG1 distribution. If we take the plots of EG1 and EG2 together we can get a first feeling about which sectors to grow/contract in the curent and next years.\n",
    "\n",
    "Nonetheless, we suspect that these plots of EG1 and EG2 can -at best- give us a rough directional indication of where earning growth/contraction to be expected. So, Let's look at a statistical description of the earning growth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_sector.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above, the unhomogenity of earning growth rate distribution is confirmed by looking at the standard deviation of the EG1 and EG2 means as well as the min and max values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will proceed to drilling down deeper by ranking the industries according the earning growth data. By doing this we expect the earning growth distribution within indutries to be more homegenous than within sectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rank Industries according to Earning Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will group the data set by Industry and calculate the average EG1 and EG2 for each Industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_industry = df.loc[:, ['Industry', 'EG1', 'EG2']].groupby('Industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe the sorted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_industry.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above we see that we have more than 200 industries in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in knowing the industries with highest growth/contraction, in the follwing we will plot the Top-10 industries with highest earning growth and the Bottom-10 industries with lowest earning growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Rank Top-10/Bottom-10 Industries according to Forward Looking Earning Growth Expectation for current year (EG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average earning grwoth for year 1 for each Industry to identify which indutries had the highest/lowest growth rates\n",
    "avgs = group_by_industry.EG1.mean()\n",
    "avgs = avgs.rename('mean EG1')\n",
    "\n",
    "#sort industries from highest to lowest EG1\n",
    "sorted_avgs = avgs.sort_values(ascending=False)\n",
    "\n",
    "top_bottom_eg1 = sorted_avgs.head(10)\n",
    "top_bottom_eg1 = top_bottom_eg1.append(sorted_avgs.tail(10))\n",
    "\n",
    "# plot the 10 industries with highest/lowest average earning growth for year 1\n",
    "top_bottom_eg1.plot(kind='barh', legend=\"Mean EG1\")\n",
    "plt.title(\"Top/Flop 10 Industries with highest/lowest average EG1\")\n",
    "plt.show()\n",
    "\n",
    "sorted_avgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Rank Top-10/Bottom-10 Industries according to Forward Looking Earning Growth Expectation for next year (EG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average earning grwoth for year 2 for each Industry to identify which indutries had the highest/lowest growth rates\n",
    "avgs = group_by_industry.EG2.mean()\n",
    "avgs = avgs.rename('mean EG2')\n",
    "\n",
    "#sort industries from highest to lowest EG2\n",
    "sorted_avgs = avgs.sort_values(ascending=False)\n",
    "\n",
    "top_bottom_eg2 = sorted_avgs.head(10)\n",
    "top_bottom_eg2 = top_bottom_eg2.append(sorted_avgs.tail(10))\n",
    "\n",
    "# plot the 10 industries with highest/lowest average earning growth for year 2\n",
    "top_bottom_eg2.plot(kind='barh', legend=\"Mean EG2\")\n",
    "plt.title(\"Top/Bottom 10 Industries with highest/lowest average EG2\")\n",
    "plt.show()\n",
    "\n",
    "sorted_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Industry\"]==\"Agriculture - Products\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Goal 2\n",
    "# Identify Potential Trade Ideas (Long/Short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preapre the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the description of our data with focus on PE1 and PE2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the PE1 column has some divisions by zero which we will try to understand in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the PE1 columns is obtained by deviding price column by the ep1 columns\n",
    "#the condition below helps us understand which stocks have 0 USD earning\n",
    "outliers = outliers.append(df[df['EPS1']==0])\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only two stocks have the problem of the division by zero. We note that both stocks are \"internet\" stocks which seem to have bad earnings per shares in period 0 and 1 but which are expected to have better earning per share in period 2. These could be stocks with \"revenue growth story\". But for now, we cannot be sure about that. We save these stocks to a new dataframe called outliers for further qualitative/quantitative analysis later. And we remove them from the dataframe so that we have a clean dataset to work with in the follwing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the stocks with infinite PE1 or PE2 ratio for later analysis\n",
    "outliers = outliers.append(df[df['EPS2']==0])\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the stocks with infinite PE1 or PE2 ratio from the dataset\n",
    "df = df[~(df['EPS1']==0)]\n",
    "df = df[~(df['EPS2']==0)]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Identify Potential Long Trade Idea for the Current Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the dataset preparation, all what we need to do is to select the industries with the highest and lowest growth from the main dataframe. \n",
    "\n",
    "For each industry with top growth we will sort the stocks within that industry from highest to lowest PE ratio. We will then export these stocks of each industry to excel on a seperate tab/sheet for further analysis. \n",
    "\n",
    "The trading/financial logic behind this approach is that the stocks with the highest PE ratios within one of the top  growth industries are potential Long Trade Ideas for two main reasons. First, the identified industry has higher grwoth than the average market growth which means that a random stock within that indutry is likely to make higher profits than the marekt average. Second, since the PE ratio is indicative for how much money investors are willing to pay for the earning of a given comppany, by selecting the highest PE ratio we identify the most loved stocks by the market within an already confirmed higher-than average growth industry.\n",
    "\n",
    "This is done in the follwoing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using openpyxl as the engine.\n",
    "writer = pd.ExcelWriter(\"long_short_outliers_year1.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name=\"Dataset\")\n",
    "\n",
    "#sort outlier from highest to lowest PE2\n",
    "outliers = outliers.sort_values(by=['PE1'], ascending=False)\n",
    "outliers.to_excel(writer, sheet_name=\"Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the PE1 sorted long trade ideas\n",
    "for x in range(5):\n",
    "    #select the industry with highest growth EG1\n",
    "    industry_df = df[df['Industry']==top_bottom_eg1.index[x]]\n",
    "    #sort the industry form highest to lowest PE1\n",
    "    industry_df = industry_df.sort_values(by=['PE1'], ascending=False)\n",
    "    #output industry dataframe to a seperate excel tab\n",
    "    sheet_name = top_bottom_eg1.index[x]\n",
    "    if (len(sheet_name) > 31):\n",
    "        sheet_name = sheet_name[0:31]\n",
    "    industry_df.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the logic explained above, stocks with lowest PE within lowest growth/ highest contraction industry are potential Short Trade Ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the PE1 sorted short trade ideas\n",
    "for x in range(5):\n",
    "    #select the industry with lowest growth EG1\n",
    "    industry_df = df[df['Industry']==top_bottom_eg1.index[len(top_bottom_eg1)-1-x]]\n",
    "    #sort the industry form lowest to highest PE1\n",
    "    industry_df = industry_df.sort_values(by=['PE1'], ascending=True)\n",
    "    #output industry dataframe to a seperate excel tab\n",
    "    sheet_name = top_bottom_eg1.index[len(top_bottom_eg1)-1-x]\n",
    "    if (len(sheet_name) > 31):\n",
    "        sheet_name = sheet_name[0:31]\n",
    "    industry_df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Identify Potential Long/Short Trade Ideas for the Next Year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the approach above, we generate an excel file with potential Long/Short Trade Ideas for the Next Year (year 2). The only difference is that instead of EG1 and PE1 as selction and sorting criteria here we use EG2 and EP2 for year 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using openpyxl as the engine.\n",
    "writer= pd.ExcelWriter(\"long_short_outliers_year2.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name=\"Dataset\")\n",
    "\n",
    "#sort outlier from highest to lowest PE2\n",
    "outliers = outliers.sort_values(by=['PE2'], ascending=False)\n",
    "outliers.to_excel(writer, sheet_name=\"Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the PE2 sorted long trade ideas\n",
    "for x in range(5):\n",
    "    #select the industry with highest growth EG2\n",
    "    industry_df = df[df['Industry']==top_bottom_eg2.index[x]]\n",
    "    #sort the industry form highest to lowest PE2\n",
    "    industry_df = industry_df.sort_values(by=['PE2'], ascending=False)\n",
    "    #output industry dataframe to a seperate excel tab\n",
    "    sheet_name = top_bottom_eg2.index[x]\n",
    "    if (len(sheet_name) > 31):\n",
    "        sheet_name = sheet_name[0:31]\n",
    "    industry_df.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the PE2 sorted short trade ideas\n",
    "for x in range(5):\n",
    "    #select the industry with lowest growth EG2\n",
    "    industry_df = df[df['Industry']==top_bottom_eg2.index[len(top_bottom_eg2)-1-x]]\n",
    "    #sort the industry form lowest to highest PE2\n",
    "    industry_df = industry_df.sort_values(by=['PE2'], ascending=True)\n",
    "    #output industry dataframe to a seperate excel tab\n",
    "    sheet_name = top_bottom_eg2.index[len(top_bottom_eg2)-1-x]\n",
    "    if (len(sheet_name) > 31):\n",
    "        sheet_name = sheet_name[0:31]\n",
    "    industry_df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Goal 3\n",
    "# Predict Stock Price "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try to predict the stock price based on the numerical variables we have. We will base the price prediction on the parameters \"Market Cap (mil)\", \"EPS0\", \"EPS1\" and \"EPS2\" since these parameters represent the genuine data as imported. All other parmeters like EG1, EG2, EP1 etc. are derived from these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a linear regression for this purpose. We don't expect great prediction precision as it is clear that predicting stock market prices with a linear model is not the smartest thing to do. Nonetheless for the purpose of this project we give it a try and see if we will get the secret to predict stock market prices :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only consider numerical variables\n",
    "X = df[[\"Market Cap (mil)\", \"EPS0\", \"EPS1\", \"EPS2\"]]\n",
    "y = df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "#Four steps:\n",
    "\n",
    "#Instantiate\n",
    "lm_model = LinearRegression(normalize=True) \n",
    "\n",
    "#Fit\n",
    "lm_model.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_test_preds = lm_model.predict(X_test) # Predictions here using X_test and lm_model\n",
    "\n",
    "#Score\n",
    "r2_test = r2_score(y_test, y_test_preds)  # Rsquared here for comparing test and preds from lm_2_model\n",
    "\n",
    "# Print r2 to see result\n",
    "r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we have an r2_test of 0.66 which is really really bad! We can try to optimize the linear regression modell by adding some categorical variables or changing the test size or the random state. But it's abious that this is not going to work! We cannot predict a highly non-linear behavior like a stock price using a linear regression.\n",
    "\n",
    "So, we will finish the project here and hope that in the next projects we will have better tools to predict stock price :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
